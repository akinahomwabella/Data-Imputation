import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder
import kagglehub
import os

# Download the dataset
path = kagglehub.dataset_download("jp797498e/twitter-entity-sentiment-analysis")

# Check if the downloaded path exists and handle the dataset
if os.path.exists(path):
    print(f"Dataset downloaded to: {path}")

    # Find the training and validation CSV files
    training_file = os.path.join(path, "twitter_training.csv")
    validation_file = os.path.join(path, "twitter_validation.csv")

    # Define custom column names since the dataset has no header
    column_names = ["Tweet ID", "entity", "sentiment", "Tweet content"]

    # Read the CSV files into DataFrames
    if os.path.exists(training_file) and os.path.exists(validation_file):
        training_data = pd.read_csv(training_file, header=None, names=column_names)
        validation_data = pd.read_csv(validation_file, header=None, names=column_names)
        print("Training and validation datasets loaded.")
    else:
        print("Error: twitter_training.csv or twitter_validation.csv not found in the downloaded dataset.")
else:
    print("Failed to download the dataset.")

# Encoding the 'entity' and 'sentiment' columns
encoder_entity = LabelEncoder()
encoder_sentiment = LabelEncoder()

# Fit on the training data and transform both datasets
training_data['entity_encoded'] = encoder_entity.fit_transform(training_data['entity']) 
validation_data['entity_encoded'] = encoder_entity.transform(validation_data['entity']) 

training_data['sentiment_encoded'] = encoder_sentiment.fit_transform(training_data['sentiment']) 
validation_data['sentiment_encoded'] = encoder_sentiment.transform(validation_data['sentiment']) 

# Define interpolation functions
def linear_interpolation(data, column):
    return data[column].interpolate(method='linear').bfill().ffill()

def spline_interpolation(data, column, order=3):
    return data[column].interpolate(method='spline', order=order).bfill().ffill()

def knn_imputation(data, column, n_neighbors=5):
    temp_data = data.copy()
    imputer = KNNImputer(n_neighbors=n_neighbors)
    temp_data[[column]] = imputer.fit_transform(temp_data[[column]])
    return temp_data[column]

# Step 2: Check for missing values
print("Missing values in training data:")
print(training_data.isnull().sum())

print("\nMissing values in validation data:")
print(validation_data.isnull().sum())

# Step 3: Apply interpolation
columns_to_interpolate = ['Tweet ID', 'entity_encoded', 'sentiment_encoded']

# Apply to training data
for column in columns_to_interpolate:
    if column in training_data.columns:
        training_data[f'{column}_linear'] = linear_interpolation(training_data, column)
        training_data[f'{column}_spline'] = spline_interpolation(training_data, column)
        training_data[f'{column}_knn'] = knn_imputation(training_data, column)

# Apply to validation data
for column in columns_to_interpolate:
    if column in validation_data.columns:
        validation_data[f'{column}_linear'] = linear_interpolation(validation_data, column)
        validation_data[f'{column}_spline'] = spline_interpolation(validation_data, column)
        validation_data[f'{column}_knn'] = knn_imputation(validation_data, column)

# Step 4: Save the cleaned datasets
training_data.to_csv('training_cleaned.csv', index=False)
validation_data.to_csv('validation_cleaned.csv', index=False)

print("\nCleaned datasets saved as 'training_cleaned.csv' and 'validation_cleaned.csv'.")
